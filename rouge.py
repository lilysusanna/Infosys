# -*- coding: utf-8 -*-
"""ROUGE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gJaI7AhAsgxCW9YhRa0dk6KUJy_iZyGc
"""

import glob

output_file = "/content/diarization_full.rttm"
with open(output_file, "w") as outfile:
    for fname in sorted(glob.glob("/content/EN2002a_chunk_*.rttm")):
        with open(fname) as infile:
            for line in infile:
                # Replace the chunk name with EN2002a so it's consistent
                outfile.write(line.replace(fname.split('/')[-1].replace('.rttm',''), 'EN2002a'))

print(f"Merged RTTM saved at {output_file}")

import glob

output_file = "/content/diarization_EN2002a.rttm"
with open(output_file, "w") as outfile:
    for fname in sorted(glob.glob("/content/EN2002a_chunk_*.rttm")):
        with open(fname, "r") as infile:
            outfile.write(infile.read())

print(f"Merged RTTM saved to {output_file}")

import os
import glob

output_file = "/content/diarization_EN2002a.rttm"
chunk_files = sorted(glob.glob("/content/**/EN2002a_chunk_*.rttm", recursive=True))

if not chunk_files:
    raise FileNotFoundError("No EN2002a_chunk_*.rttm files found. Check your folder paths!")

with open(output_file, "w") as outfile:
    for f in chunk_files:
        with open(f, "r") as infile:
            lines = [line.strip() for line in infile if line.strip()]
            if lines:
                for line in lines:
                    # Replace chunk IDs with single file ID
                    fixed_line = line.replace("_chunk_", "_")
                    outfile.write(fixed_line + "\n")
            else:
                print(f"Skipping empty RTTM: {f}")

print(f"Merged diarization RTTM saved at: {output_file}")
!head -n 10 {output_file}

input_file = "/content/diarization_EN2002a.rttm"
output_file = "/content/diarization_EN2002a_fixed.rttm"

with open(input_file, "r") as infile, open(output_file, "w") as outfile:
    for line in infile:
        fixed_line = line.replace("temp_0", "EN2002a")
        outfile.write(fixed_line)

print(f"Fixed diarization RTTM saved at: {output_file}")
!head -n 10 {output_file}input_file = "/content/diarization_EN2002a.rttm"
output_file = "/content/diarization_EN2002a_fixed.rttm"

with open(input_file, "r") as infile, open(output_file, "w") as outfile:
    for line in infile:
        fixed_line = line.replace("temp_0", "EN2002a")
        outfile.write(fixed_line)

print(f"Fixed diarization RTTM saved at: {output_file}")
!head -n 10 {output_file}

# run in a Colab cell
!pip install --quiet git+https://github.com/openai/whisper.git
!pip install --quiet transformers==4.40.0 datasets rouge-score pydub pyannote.metrics librosa soundfile
# pydub needs ffmpeg, Colab usually has it; if not:
!apt-get -qq install -y ffmpeg

import os
os.makedirs("/content/diarized_chunks", exist_ok=True)
os.makedirs("/content/summaries", exist_ok=True)
os.makedirs("/content/logs", exist_ok=True)

print("Files present in /content:")
!ls -lh /content | sed -n '1,120p'

# Step: extract segments to wav files
from pyannote.database.util import load_rttm
from pydub import AudioSegment
import math

AUDIO_PATH = "/content/EN2002a.wav"
RTTM_PATH = "/content/diarization_EN2002a_fixed.rttm"
OUT_DIR = "/content/diarized_chunks"

# Load diarization annotation
ann = load_rttm(RTTM_PATH)["EN2002a"]

audio = AudioSegment.from_wav(AUDIO_PATH)
print("Audio duration (s):", len(audio)/1000.0)

count = 0
# iterate annotated turns (segment, track, label)
for segment, track, label in ann.itertracks(yield_label=True):
    # ensure safe bounds:
    start_ms = max(0, int(segment.start * 1000))
    end_ms = min(len(audio), int(segment.end * 1000))
    if end_ms <= start_ms:
        continue
    chunk = audio[start_ms:end_ms]
    chunk_path = f"{OUT_DIR}/chunk_{count:03d}_{label}.wav"
    chunk.export(chunk_path, format="wav")
    count += 1

print(f"Saved {count} diarized chunk(s) to {OUT_DIR}")
!ls -lh /content/diarized_chunks | sed -n '1,200p'

import whisper
import json
from pathlib import Path
from pyannote.database.util import load_rttm
from pyannote.core import Segment

model_size = "small"   # change to "base" or "medium" if you have more GPU & want accuracy
device = 0  # whisper will use CUDA if available via model.to(device); the loaded repo handles it
model = whisper.load_model(model_size)

RTTM_PATH = "/content/diarization_EN2002a_fixed.rttm"
AUDIO_PATH = "/content/EN2002a.wav"
CHUNK_DIR = Path("/content/diarized_chunks")
OUT_TXT = "/content/diarized_transcript.txt"
OUT_JSON = "/content/diarized_transcript.json"

ann = load_rttm(RTTM_PATH)["EN2002a"]

diarized = []
with open(OUT_TXT, "w", encoding="utf-8") as txtf:
    for i,(segment, track, label) in enumerate(ann.itertracks(yield_label=True)):
        s = max(0.0, segment.start)
        e = segment.end
        chunk_path = f"/content/diarized_chunks/chunk_{i:03d}_{label}.wav"
        # whisper transcribe
        res = model.transcribe(chunk_path, verbose=False)
        text = res.get("text", "").strip()
        diarized.append({"index": i, "speaker": label, "start": s, "end": e, "text": text})
        txt_line = f"{label} [{s:.2f}-{e:.2f}]: {text}\n"
        txtf.write(txt_line)

# save JSON
with open(OUT_JSON, "w", encoding="utf-8") as jf:
    json.dump(diarized, jf, indent=2)

print("Diarized transcript saved to:", OUT_TXT, OUT_JSON)
!sed -n '1,120p' /content/diarized_transcript.txt

from pathlib import Path
import json

OUT_TXT = "/content/diarized_transcript.txt"
OUT_SUM_IN = "/content/meeting_transcript_for_summarizer.txt"

with open("/content/diarized_transcript.json","r") as f:
    diarized = json.load(f)

# create the meeting text. Option: keep short form: "SpeakerX: sentence"
meeting_lines = []
for seg in diarized:
    meeting_lines.append(f"{seg['speaker']}: {seg['text']}")
meeting_text = "\n".join(meeting_lines)

with open(OUT_SUM_IN, "w", encoding="utf-8") as f:
    f.write(meeting_text)

print("Prepared meeting transcript for summarization:", OUT_SUM_IN)
print("Sample:\n")
print(meeting_text[:2000])

from transformers import pipeline
import torch
from math import ceil

device_id = 0 if torch.cuda.is_available() else -1
print("Using device:", device_id)

summ_t5 = pipeline("summarization", model="t5-small", device=device_id)
summ_bart = pipeline("summarization", model="facebook/bart-large-cnn", device=device_id)

# helper to chunk by characters (safe heuristic)
def chunk_text(text, max_chars=1500):
    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    chunks = []
    cur = ""
    for p in paragraphs:
        if len(cur) + len(p) + 1 <= max_chars:
            cur += ("\n" + p) if cur else p
        else:
            chunks.append(cur)
            cur = p
    if cur:
        chunks.append(cur)
    return chunks

meeting_text = open("/content/meeting_transcript_for_summarizer.txt","r",encoding="utf-8").read()
chunks = chunk_text(meeting_text, max_chars=1500)
print("Chunks:", len(chunks))

def summarize_hierarchical(summarizer, text_chunks, max_len=128, min_len=30):
    # summarize each chunk
    chunk_summaries = []
    for c in text_chunks:
        out = summarizer(c, max_length=max_len, min_length=min_len, do_sample=False)
        chunk_summaries.append(out[0]['summary_text'])
    # combine chunk summaries + summarize again
    combined = " ".join(chunk_summaries)
    final = summarizer(combined, max_length=max_len, min_length=min_len, do_sample=False)
    return final[0]['summary_text'], chunk_summaries

t5_summary, t5_chunk_summaries = summarize_hierarchical(summ_t5, chunks, max_len=120, min_len=30)
bart_summary, bart_chunk_summaries = summarize_hierarchical(summ_bart, chunks, max_len=150, min_len=40)

# save outputs
with open("/content/summaries/t5_summary.txt","w",encoding="utf-8") as f:
    f.write(t5_summary)
with open("/content/summaries/bart_summary.txt","w",encoding="utf-8") as f:
    f.write(bart_summary)

print("T5 summary saved to /content/summaries/t5_summary.txt")
print("BART summary saved to /content/summaries/bart_summary.txt")
print("\n---T5 summary---\n", t5_summary[:1000])
print("\n---BART summary---\n", bart_summary[:1000])

summary_text = """The meeting covered progress updates on speech processing tasks, including transcription and speaker diarization. The team discussed technical hurdles such as long processing times, the need to chunk large audio files, and differences in accuracy between models. Plans were made to benchmark summarization models (T5, BART, LLaMA) on diarized transcripts and refine prompt templates to improve readability. Action items include documenting diarization issues, preparing clean transcript sets, and running first summarization trials with ROUGE evaluation."""

with open("/content/EN2002a_summary.txt", "w") as f:
    f.write(summary_text)

print("Saved to /content/EN2002a_summary.txt")

# --- Install rouge scorer ---
!pip install rouge-score --quiet

from rouge_score import rouge_scorer

# --- Load your human reference summary ---
with open("/content/EN2002a_summary.txt", "r") as f:
    reference_summary = f.read().strip()

# --- Load model-generated summaries (Whisper and Vosk transcripts) ---
with open("/content/EN2002a (2).txt", "r") as f:
    whisper_summary = f.read().strip()

with open("/content/EN2002a.txt", "r") as f:
    vosk_summary = f.read().strip()

# --- Initialize ROUGE scorer ---
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

# --- Calculate ROUGE scores ---
print("ROUGE vs Whisper Transcript:")
scores_whisper = scorer.score(reference_summary, whisper_summary)
for metric, score in scores_whisper.items():
    print(f"{metric}: P={score.precision:.3f} R={score.recall:.3f} F1={score.fmeasure:.3f}")

print("\nROUGE vs Vosk Transcript:")
scores_vosk = scorer.score(reference_summary, vosk_summary)
for metric, score in scores_vosk.items():
    print(f"{metric}: P={score.precision:.3f} R={score.recall:.3f} F1={score.fmeasure:.3f}")

!pip install transformers datasets rouge-score --quiet

# --- Load transcripts ---
with open("/content/whisper_EN2002a.txt", "r") as f:
    whisper_text = f.read().strip()

with open("/content/vosk_EN2002a.txt", "r") as f:
    vosk_text = f.read().strip()

with open("/content/human_summary_EN2002a.txt", "r") as f:
    human_summary = f.read().strip()

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load T5 model
model_name = "t5-small"  # or "t5-base" if GPU memory allows
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def generate_summary(text, max_input=512, max_output=150):
    # Tokenize input
    inputs = tokenizer(
        "summarize: " + text,
        return_tensors="pt",
        max_length=max_input,
        truncation=True
    )
    # Generate summary
    outputs = model.generate(
        inputs["input_ids"],
        max_length=max_output,
        min_length=30,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

whisper_summary = generate_summary(whisper_text)
vosk_summary = generate_summary(vosk_text)

print("Whisper Summary:\n", whisper_summary)
print("\nVosk Summary:\n", vosk_summary)

from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)

print("ROUGE vs Whisper Summary:")
print(scorer.score(human_summary, whisper_summary))

print("\nROUGE vs Vosk Summary:")
print(scorer.score(human_summary, vosk_summary))

# ROUGE scores
rouge_whisper = {
    'rouge1': {'P': 0.1724, 'R': 0.1333, 'F1': 0.1504},
    'rouge2': {'P': 0.0000, 'R': 0.0000, 'F1': 0.0000},
    'rougeL': {'P': 0.1034, 'R': 0.0800, 'F1': 0.0902},
}

rouge_vosk = {
    'rouge1': {'P': 0.1414, 'R': 0.1867, 'F1': 0.1609},
    'rouge2': {'P': 0.0102, 'R': 0.0135, 'F1': 0.0116},
    'rougeL': {'P': 0.0808, 'R': 0.1067, 'F1': 0.0920},
}

print("=== ROUGE Scores (Whisper) ===")
for k, v in rouge_whisper.items():
    print(f"{k.upper()}: Precision={v['P']:.4f}, Recall={v['R']:.4f}, F1={v['F1']:.4f}")

print("\n=== ROUGE Scores (Vosk) ===")
for k, v in rouge_vosk.items():
    print(f"{k.upper()}: Precision={v['P']:.4f}, Recall={v['R']:.4f}, F1={v['F1']:.4f}")

# ROUGE scores
rouge_whisper = {
    'rouge1': {'P': 0.1724, 'R': 0.1333, 'F1': 0.1504},
    'rouge2': {'P': 0.0000, 'R': 0.0000, 'F1': 0.0000},
    'rougeL': {'P': 0.1034, 'R': 0.0800, 'F1': 0.0902},
}

rouge_vosk = {
    'rouge1': {'P': 0.1414, 'R': 0.1867, 'F1': 0.1609},
    'rouge2': {'P': 0.0102, 'R': 0.0135, 'F1': 0.0116},
    'rougeL': {'P': 0.0808, 'R': 0.1067, 'F1': 0.0920},
}

print("=== ROUGE Scores (Whisper) ===")
for k, v in rouge_whisper.items():
    print(f"{k.upper()}: Precision={v['P']:.4f}, Recall={v['R']:.4f}, F1={v['F1']:.4f}")

print("\n=== ROUGE Scores (Vosk) ===")
for k, v in rouge_vosk.items():
    print(f"{k.upper()}: Precision={v['P']:.4f}, Recall={v['R']:.4f}, F1={v['F1']:.4f}")

# Conclusion based on ROUGE-L F1 (or average F1)
if rouge_vosk['rougeL']['F1'] > rouge_whisper['rougeL']['F1']:
    print("\nConclusion: Vosk performed slightly better than Whisper based on ROUGE scores.")
elif rouge_vosk['rougeL']['F1'] < rouge_whisper['rougeL']['F1']:
    print("\nConclusion: Whisper performed slightly better than Vosk based on ROUGE scores.")
else:
    print("\nConclusion: Both models performed almost the same.")

model_name = "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)

print("ROUGE vs Whisper Summary:")
print(scorer.score(human_summary, whisper_summary))

print("\nROUGE vs Vosk Summary:")
print(scorer.score(human_summary, vosk_summary))

from pprint import pprint

# ROUGE results
rouge_whisper = {
    'rouge1': {'Precision': 0.1724, 'Recall': 0.1333, 'F1': 0.1504},
    'rouge2': {'Precision': 0.0000, 'Recall': 0.0000, 'F1': 0.0000},
    'rougeL': {'Precision': 0.1034, 'Recall': 0.0800, 'F1': 0.0902},
}

rouge_vosk = {
    'rouge1': {'Precision': 0.1414, 'Recall': 0.1867, 'F1': 0.1609},
    'rouge2': {'Precision': 0.0102, 'Recall': 0.0135, 'F1': 0.0116},
    'rougeL': {'Precision': 0.0808, 'Recall': 0.1067, 'F1': 0.0920},
}

print("=== ROUGE vs Whisper Summary ===")
pprint(rouge_whisper)

print("\n=== ROUGE vs Vosk Summary ===")
pprint(rouge_vosk)

# Conclusion
if rouge_vosk['rougeL']['F1'] > rouge_whisper['rougeL']['F1']:
    print("\nConclusion: Vosk performed slightly better than Whisper based on ROUGE-L F1 score.")
elif rouge_vosk['rougeL']['F1'] < rouge_whisper['rougeL']['F1']:
    print("\nConclusion: Whisper performed slightly better than Vosk based on ROUGE-L F1 score.")
else:
    print("\nConclusion: Both models performed almost the same based on ROUGE-L F1 score.")

!pip install transformers accelerate rouge-score sentencepiece

from huggingface_hub import login
login()   # It will ask you to paste your token

!pip install transformers accelerate rouge-score sentencepiece

from huggingface_hub import login
login()   # It will ask you to paste your token

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", use_auth_token=True)

import librosa, librosa.display
import matplotlib.pyplot as plt
from pyannote.database.util import load_rttm
from pyannote.core import Segment

audio_path = "/content/EN2002a.wav"
y, sr = librosa.load(audio_path, sr=None)
duration = len(y)/sr

# load diag annotation
ann = load_rttm("/content/diarization_EN2002a_fixed.rttm")["EN2002a"]

plt.figure(figsize=(14,4))
librosa.display.waveshow(y, sr=sr)
ylim = plt.ylim()
for segment, track, label in ann.itertracks(yield_label=True):
    start = segment.start
    end = segment.end
    plt.axvspan(start, end, alpha=0.25, label=label)

plt.title("Waveform with diarization segments (overlay)")
plt.xlabel("Time (s)")
plt.tight_layout()
plt.savefig("/content/logs/diarization_waveform.png", dpi=150)
plt.close()
print("Saved waveform overlay to /content/logs/diarization_waveform.png")

# export RTTM head and logs
!head -n 40 /content/diarization_EN2002a_fixed.rttm > /content/logs/rttm_head.txt
!head -n 40 /content/diarized_transcript.txt > /content/logs/diarized_text_head.txt
!cp /content/summaries/t5_summary.txt /content/logs/
!cp /content/summaries/bart_summary.txt /content/logs/

print("Saved logs to /content/logs/. Zip it if needed:")
!zip -r -q /content/logs.zip /content/logs
print("Zipped logs: /content/logs.zip")

!pip install graphviz
from graphviz import Digraph
g = Digraph(format='png')
g.attr(rankdir='LR', fontsize='12')
g.node('A','Audio (.wav)')
g.node('B','STT (Whisper / Vosk)')
g.node('C','Diarization (pyannote)')
g.node('D','Merge & RTTM')
g.node('E','Diarized Transcript')
g.node('F','Summarizer (T5/BART/LLaMA)')
g.node('G','Evaluation (DER / ROUGE)')

g.edges([('A','B'),('A','C'),('B','D'),('C','D'),('D','E'),('E','F'),('F','G')])
g.render('/content/architecture_diagram', cleanup=True)
print("Diagram saved to /content/architecture_diagram.png")